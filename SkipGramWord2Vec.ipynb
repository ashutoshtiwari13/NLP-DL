{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConsidering the following statement :\\n\"the first coding homework was fun\"\\n\\nwith a window size of 1 ,we have the dataset:\\n([the,coding],first),([lot,fun],of)....\\n\\nThe skipgram model tries to predict each context word from its target worf,\\nand so the task becomes to predict \\'the \\'and \\'coding\\' from first.\\'lot\\'\\nand \\'fun\\' from \\'of\\' and so on.\\n\\nThe dataset now is:\\n(first,the),(first,coding),(of,lot),(of,fun)....\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the skip gram model which is a naive model for understanding NLP basics\n",
    "import math \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "Considering the following statement :\n",
    "\"the first coding homework was fun\"\n",
    "\n",
    "with a window size of 1 ,we have the dataset:\n",
    "([the,coding],first),([lot,fun],of)....\n",
    "\n",
    "The skipgram model tries to predict that each context word from its target word,\n",
    "and so the task becomes to predict 'the 'and 'coding' from first.'lot'\n",
    "and 'fun' from 'of' and so on.\n",
    "\n",
    "The dataset now is:\n",
    "(first,the),(first,coding),(of,lot),(of,fun)....\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the constants\n",
    "batch_size=128\n",
    "vocabulary_size=50000\n",
    "embedding_size=128 #dimensuon of the embedded vector\n",
    "num_sampled=64 #Numbe rof negative examples to sample\n",
    "'''\n",
    "load_data loads the already preprocessed training and val data\n",
    "\n",
    "train_data is a list of {batch_input,batch_labels} pairs.\n",
    "val data is a list of all validation inputs.\n",
    "reverse_dictionary is a python dict from word index to word\n",
    "'''\n",
    "\n",
    "train_data,val_data,reverse_dictionary=load_data()\n",
    "print(\"Number of training examples:\",len(train_data)*batch_size)\n",
    "print(\"Number of validation examples:\",len(val_data))\n",
    "\n",
    "def skipgram():\n",
    "    batch_inputs=tf.placeholder(tf.int32,shape=[batch_size,])\n",
    "    batch_labels=tf.placeholder(tf.int32,shape=[batch_size,1])\n",
    "    val_dataset=tf.constant(val_data,dtype=tf.int32)\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(\"word2vec\") as scope:\n",
    "        embeddings=tf.Variable(tf.random_uniform([vocabulary_size,\n",
    "                                                 embedding_size],\n",
    "                                                -1.0,1.0))\n",
    "        batch_embeddings=tf.nn.embedding_lookup(embeddings,batch_inputs)\n",
    "        \n",
    "        \n",
    "        weights=tf.Variable(tf.truncated_normal([vocabulary_size,\n",
    "                                                embedding_size],\n",
    "                            stddev=1.0/math.sqrt(embedding_size)))\n",
    "        biases=tf.Variable(tf.zeros([vocabulary_size]))\n",
    "        \n",
    "        loss=tf.reduce_mean(tf.nn.nce_loss(weights=weights,\n",
    "                           biases=biases,\n",
    "                           labels=batch_labels,\n",
    "                           inputs=batch_inputs,\n",
    "                           num_sampled=num_sampled,\n",
    "                           num_classes=vocabulary_size))\n",
    "        '''\n",
    "        num_sampled for automatic addition of context and noisy samples instead of only\n",
    "        context words and target words.num_sampled=64, will look up 64 words which are not \n",
    "        in our sample training set and which are noise words and serve as negative samples \n",
    "        so that network knows which words are actaully \n",
    "        context words and which are not\n",
    "        '''\n",
    "         \n",
    "        \n",
    "        norm=tf.sqrt(tf.reduce_mean(tf.square(embeddings),1,keep_dims=True))\n",
    "        normalized_embeddings=embedding/norm\n",
    "        \n",
    "        val_embeddings=tf.nn.embedding_lookup(normalized_embeddings,val_dataset)\n",
    "        similarity=tf.matmul(val_embeddings,normalized_embeddings,tranpose_b=True)\n",
    "        \n",
    "        return batch_inputs,batch_labels,normalized_embeddings,loss,similarity\n",
    "    \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def run():\n",
    "    \n",
    "    batch_inputs,batch_labels,normalized_embeddings,loss,similarity=skipgram()\n",
    "    optimizer=tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        average_loss=0.0\n",
    "        for step ,batch_data in enumerate(train_data):\n",
    "            inputs,labels=batch_data\n",
    "            feed_dict={batch_inputs:inputs,batch_labels:labels}\n",
    "            \n",
    "            _,loss_val=sess.run([optimizer,loss],feed_dict)\n",
    "            avearge_loss+=loss_val\n",
    "            \n",
    "            if step%1000==0:\n",
    "                if step>0:\n",
    "                    average_loss/=1000\n",
    "                    print('loss at iteration',step,\":\",average_loss)\n",
    "                    average_loss=0\n",
    "                    \n",
    "            if step%5000==0:\n",
    "                sim=similarity.eval()  #similar to call session.run on similarity and fetching back\n",
    "                for i in xrange(len(val_data)):\n",
    "                    top_k=8\n",
    "                    nearest=(sim[i,:]).argsort()[1:top_k+1]\n",
    "                    print_closest_words(val_data[i],nearest,reverse_dictionary)\n",
    "         \n",
    "        final_embeddings= normalized_embedding.eval()          \n",
    "                    \n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "#lets start training\n",
    "final_embeddings=run()\n",
    "    \n",
    "#visualizinng the embeddings\n",
    "visualize_embeddings(final_embeddings,reverse_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
